from typing import List, Optional
from openai import OpenAI
import os
from dotenv import load_dotenv
import json
from backend.types import TaskMetadata, TaskJudgment, SubtaskMetadata, SubtaskJudgment
from fastapi import HTTPException

# Load environment variables from .env file
load_dotenv()

# --- Constants ---
DEFAULT_MODEL = "gpt-4"

# --- Shared LLM client accessor ---
def get_client():
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        raise ValueError(
            "OpenAI API key not found. Please set the OPENAI_API_KEY environment variable. "
            "You can get an API key from https://platform.openai.com/api-keys"
        )
    return OpenAI(api_key=openai_api_key)

# --- LLM-enabled functions ---
def extract_task(state) -> TaskMetadata:
    """
    Use LLM to extract the main task, assess confidence, raise concerns, and generate clarifying questions.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
    You are an expert task manager assistant.

    Your job is to extract a single main task from the user's input. Focus on *only* the main task. Do not speculate about subtasks or missing information.

    Specifically:
    - Extract the top-level task only, based solely on the user input.
    - Set `is_subtaskable` to True if the task *could* be broken down into parts — but do not list or suggest any.
    - Do not include concerns or questions about missing subtasks or steps — those will be handled later in the workflow.
    - Only raise concerns or ask questions if the parent task itself is vague or ambiguous.

    Respond using the following strict JSON format:
    {
    "task": <string>,
    "confidence": <float between 0 and 1>,
    "concerns": [<string>, ...],
    "questions": [<string>, ...],
    "is_subtaskable": <boolean>
    }
    </system_prompt>
    """


    user_prompt = f"""
    <user_prompt>
        {state.input}
    </user_prompt>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    try:
        content = response.choices[0].message.content.strip()
        return TaskMetadata(**json.loads(content))
    except Exception as e:
        return TaskMetadata(
            task=state.input.strip(),
            confidence=0.0,
            concerns=["Unable to parse task extraction response"],
            questions=[],
            is_subtaskable=False
        )

def judge_task(metadata: TaskMetadata) -> TaskJudgment:
    """
    Determine if the extracted task is clearly defined and actionable.
    Uses task confidence, concerns, and clarification questions as context.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
    You are a quality control specialist reviewing a proposed task generated by an AI assistant.

    Your job is to determine if the task is clearly defined, specific enough to take action on, and well scoped.

    Evaluation Criteria:
    - A confidence score below 0.7 should make you cautious.
    - If there are concerns or clarification questions, the task may be vague or incomplete.
    - The user will have an opportunity to create subtasks later. You only need to judge the task itself.
    - Use your best judgment to determine whether the task is ready to proceed or needs clarification.

    Edge Case:
    - If the task is vague and the assistant failed to generate clarifying questions, you must add at least one question in your reason.

    Always respond using the following JSON format:
    {
    "judgment": "pass" or "fail",
    "reason": "<clarification or explanation if needed>"
    }
    </system_prompt>
    """

    user_prompt = f"""
    <user_prompt>
        <task>{metadata.task}</task>
        <confidence>{metadata.confidence}</confidence>

        <concerns>
        {chr(10).join(metadata.concerns) if metadata.concerns else 'None'}
        </concerns>

        <questions>
        {chr(10).join(metadata.questions) if metadata.questions else 'None'}
        </questions>
    </user_prompt>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    try:
        content = response.choices[0].message.content.strip()
        return TaskJudgment(**json.loads(content))
    except Exception:
        return TaskJudgment(
            judgment="fail",
            reason="Task judgment failed: unable to parse task judgment response."
        )

def judge_subtasks(metadata: TaskMetadata, subtasks: SubtaskMetadata) -> SubtaskJudgment:
    """
    Evaluate whether the generated subtasks represent a complete and logical decomposition of the main task.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
    You are a quality control specialist reviewing a proposed list of subtasks generated by an AI assistant.
    Your job is to determine if the subtasks are a clear, complete, and logical decomposition of the main task.

    Review Criteria:
    - Confidence score below 0.7 should make you cautious — don't approve unless the structure is still obviously solid.
    - Check for vague, redundant, overlapping, or misaligned subtasks.
    - Consider concerns and questions — if these are significant or unanswered, that's a warning sign.
    - If the subtasks are ambiguous and no clarifying questions were generated, include at least one question in your reason.
    - If subtasks seem usable and editable by the user, a 'pass' is acceptable — even if not perfect.

    Always respond using the following JSON format:
    {
    "judgment": "pass" or "fail",
    "reason": "<clarification or explanation if needed>"
    }
    </system_prompt>
    """

    user_prompt = f"""
    <user_prompt>
        <task>{metadata.task}</task>

        <subtasks>
        {chr(10).join(subtasks.subtasks) if subtasks.subtasks else 'None'}
        </subtasks>

        <confidence>{subtasks.confidence}</confidence>

        <concerns>
        {chr(10).join(subtasks.concerns) if subtasks.concerns else 'None'}
        </concerns>

        <questions>
        {chr(10).join(subtasks.questions) if subtasks.questions else 'None'}
        </questions>
    </user_prompt>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    try:
        content = response.choices[0].message.content.strip()
        return SubtaskJudgment(**json.loads(content))
    except Exception:
        return SubtaskJudgment(
            judgment="fail",
            reason="Subtask judgment failed: unable to parse subtask judgment response."
        )

def save_task_to_db(task: str, subtasks: Optional[List[str]] = None):
    subtasks = subtasks or []
    print(f"[SAVE] Task: {task}\n[SUBTASKS]\n" + chr(10).join(f"- {s}" for s in subtasks))
    return {
        "status": "saved"
    }

def generate_subtasks(metadata: TaskMetadata) -> SubtaskMetadata:
    """
    Use LLM to propose subtasks for a given task and identify missing information.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
    You are an expert task planning assistant.

    Your job is to break down a single task into a set of **clear, unambiguous subtasks**. Only proceed if you have enough information to do so confidently. If critical context is missing, **ask clarifying questions instead of guessing**.

    Instructions:
    - Generate subtasks only if you are at least moderately confident (0.7 or higher).
    - Do not speculate about vague or unspecified outcomes.
    - If information is missing, prioritize writing well-formed clarification questions.
    - Add any concerns that would help the user understand what is unclear.

    Always respond using the following JSON format:
    {
    "subtasks": [<string>, ...],
    "confidence": <float between 0 and 1>,
    "concerns": [<string>, ...],
    "questions": [<string>, ...]
    }
    </system_prompt>
    """

    user_prompt = f"""
    <user_prompt>
        {metadata.task}
    </user_prompt>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    try:
        content = response.choices[0].message.content.strip()
        return SubtaskMetadata(**json.loads(content))
    except Exception:
        return SubtaskMetadata(
            subtasks=[],
            confidence=0.0,
            concerns=["Unable to parse subtask generation response"],
            questions=[]
        )

def create_task(task: str, subtasks: Optional[List[str]] = None) -> dict:
    """
    Create a new task with optional subtasks.
    Currently just logs the task and subtasks.
    TODO: Implement actual task storage
    """
    subtasks = subtasks or []
    print(f"[SAVE] Task: {task}\n[SUBTASKS]\n" + chr(10).join(f"- {s}" for s in subtasks))
    return {
        "status": "saved",
        "task": task,
        "subtasks": subtasks
    }

def generate_task_clarification_prompt(metadata, judgment, context_type: str) -> str:
    """
    Generate a human-friendly prompt for task/subtask clarification.
    Uses concerns and questions from metadata to create a clear message.
    """
    concerns = metadata.concerns or []
    questions = metadata.questions or []

    lines = []

    # Determine the goal of the interaction
    if not concerns and not questions:
        lines.append(f"Here's what I came up with for your {context_type}. Does this look right to you?")
    else:
        lines.append(f"I need your help clarifying your {context_type}.")

        if concerns:
            lines.append("\nHere are a few concerns I have:")
            lines.extend(f"- {c}" for c in concerns)

        if questions:
            lines.append("\nCould you please clarify:")
            lines.extend(f"- {q}" for q in questions)

    return "\n".join(lines).strip()

def retry_task_with_feedback(state) -> TaskMetadata:
    """
    Use LLM to refine the task based on user feedback.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
        You are an expert task manager assistant.
        Your job is to refine the task based on the user's feedback.
        Consider the feedback carefully and update the task accordingly.
        Assess your confidence in the refined task, list any remaining concerns,
        and generate any additional clarification questions if needed.

        Always respond using the following JSON format:
        {
        "task": <string>,
        "confidence": <float between 0 and 1>,
        "concerns": [<string>, ...],
        "questions": [<string>, ...]
        }
    </system_prompt>
    """

    user_prompt = f"""
    <user_prompt>
        <original_task>{state.task_metadata.task}</original_task>
        <user_feedback>{state.user_feedback}</user_feedback>
    </user_prompt>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    try:
        content = response.choices[0].message.content.strip()
        return TaskMetadata(**json.loads(content))
    except Exception as e:
        return TaskMetadata(
            task=state.task_metadata.task,
            confidence=0.0,
            concerns=["Unable to parse task refinement response"],
            questions=[]
        )

def retry_subtasks_with_feedback(state) -> SubtaskMetadata:
    """
    Use LLM to refine subtasks based on user feedback.
    """

    client = get_client()

    system_msg = """
    <system_prompt>
    You are an expert task planning assistant.

    Your job is to refine the current list of subtasks based on user feedback.
    - Apply the feedback carefully to modify the existing subtasks.
    - Preserve the structure where possible; only change what's necessary.
    - If any concerns remain, include them.
    - If anything is unclear, include clarifying questions.

    Always respond using the following JSON format:
    {
      "subtasks": [<string>, ...],
      "confidence": <float>,
      "concerns": [<string>, ...],
      "questions": [<string>, ...]
    }
    </system_prompt>
    """

    original_prompt = f"<original_prompt>{state.last_user_message}</original_prompt>" if state.last_user_message else ""

    # Handle case when subtask_metadata is None
    original_subtasks = state.subtask_metadata.subtasks if state.subtask_metadata else []

    user_msg = f"""
    <user_prompt>
    <task>{state.task_metadata.task}</task>

    <original_subtasks>
    {chr(10).join(original_subtasks)}
    </original_subtasks>

    <user_feedback>{state.user_feedback}</user_feedback>

    {original_prompt}
    </user_prompt>
    """

    try:
        response = client.chat.completions.create(
            model=DEFAULT_MODEL,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
            temperature=0,
            response_format={"type": "json_object"}
        )
        return SubtaskMetadata(**json.loads(response.choices[0].message.content))

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"retry_subtasks_with_feedback failed: {str(e)}")
