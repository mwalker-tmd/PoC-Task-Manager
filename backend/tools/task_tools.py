from typing import List, Optional
from openai import OpenAI
import os
from dotenv import load_dotenv
import json
from langgraph.errors import GraphInterrupt
from langgraph.types import Interrupt
from backend.types import TaskMetadata, TaskJudgment, SubtaskMetadata, SubtaskJudgment, SubtaskDecision

# Load environment variables from .env file
load_dotenv()

# --- Constants ---
DEFAULT_MODEL = "gpt-4"

# --- Shared LLM client accessor ---
def get_client():
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        raise ValueError(
            "OpenAI API key not found. Please set the OPENAI_API_KEY environment variable. "
            "You can get an API key from https://platform.openai.com/api-keys"
        )
    return OpenAI(api_key=openai_api_key)

# --- LLM-enabled functions ---
def extract_task(state) -> TaskMetadata:
    """
    Use LLM to extract the main task, assess confidence, raise concerns, and generate clarifying questions.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
        You are an expert task manager assistant.
        Your job is to extract a single task from the user's input, assess how confident you are,
        list any concerns, and, if you want clarification from the user, generate clarification questions.

        Always respond using the following JSON format:
        {
        "task": <string>,
        "confidence": <float between 0 and 1>,
        "concerns": [<string>, ...],
        "questions": [<string>, ...]
        }
    </system_prompt>
    """

    user_prompt = f"""
    <user_prompt>
        {state.input}
    </user_prompt>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    
    try:
        content = response.choices[0].message.content.strip()
        return TaskMetadata(**json.loads(content))
    except Exception as e:
        return TaskMetadata(
            task=state.input.strip(),
            confidence=0.0,
            concerns=["Unable to parse task extraction response"],
            questions=[]
        )

def clarify(missing_info: List[str]) -> dict:
    """
    Generate clarification questions for missing task info using an LLM.
    """
    client = get_client()
    if not missing_info:
        return {"questions": []}

    prompt = f"""
    A task is missing the following pieces of information: {', '.join(missing_info)}.
    Write a short clarification question for each.
    """
    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": "You generate clarification questions for missing task details."},
            {"role": "user", "content": prompt}
        ]
    )
    content = response.choices[0].message.content.strip()
    questions = [line.lstrip("- ").strip() for line in content.splitlines() if line.strip()]
    return {
        "questions": questions
    }

def review(task: str, subtasks: Optional[List[str]] = None) -> dict:
    """
    Present the task and subtasks back to the user for review.
    """
    return {
        "task": task,
        "subtasks": subtasks or [],
        "message": "Here's what I've extracted. Let me know if you'd like to change anything."
    }

def judge_task(metadata: TaskMetadata) -> TaskJudgment:
    """
    Determine if the extracted task is clearly defined and actionable.
    Uses task confidence, concerns, and clarification questions as context.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
        You are a quality control specialist reviewing a proposed task generated by an AI assistant.

        Your job is to decide if the proposed task is clearly defined, actionable, and appropriately scoped.

        Consider the following:
        - Confidence score below 0.7 should make you cautious.
        - If concerns or clarification questions are present, it's more likely the task is vague.
        - However, you must use your own judgment to decide if the task can proceed or not.

        If the task is too ambiguous and no questions were suggested by the assistant, 
        you must generate at least one clarifying question for the user and include it in your reason.

        Always respond using the following JSON format:
        {
        "judgment": "pass" or "fail",
        "reason": "<clarification or explanation if needed>"
        }
        </system_prompt>
    """

    user_prompt = f"""
    <user_prompt>
        <task>{metadata.task}</task>
        <confidence>{metadata.confidence}</confidence>

        <concerns>
        {chr(10).join(metadata.concerns) if metadata.concerns else 'None'}
        </concerns>

        <questions>
        {chr(10).join(metadata.questions) if metadata.questions else 'None'}
        </questions>
    </user_prompt>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    
    try:
        content = response.choices[0].message.content.strip()
        return TaskJudgment(**json.loads(content))
    except Exception:
        return TaskJudgment(
            judgment="fail",
            reason="Task judgment failed: unable to parse task judgment response."
        )

def judge_subtasks(metadata: TaskMetadata, subtasks: SubtaskMetadata) -> SubtaskJudgment:
    """
    Evaluate whether the generated subtasks represent a complete and logical decomposition of the main task.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
        You are a quality control specialist reviewing a proposed list of subtasks generated by an AI assistant.
        Your job is to determine if a list of subtasks correctly and completely decomposes the main task.

        If the subtasks are incomplete, overlapping, redundant, or not well-aligned with the task, return "fail" and explain why.

        Consider the following:
        - Confidence score below 0.7 should make you cautious.
        - If concerns or clarification questions are present, it's more likely the subtasks are vague and/or incomplete.
        - However, you must use your own judgment to decide if the task can proceed or not.

        If the subtasks are too ambiguous and no questions were suggested by the assistant, you 
        must generate at least one clarifying question for the user and include it in your reason.

        Always respond using the following JSON format:
        {
        "judgment": "pass" or "fail",
        "reason": "<clarification or explanation if needed>"
        }
        </system_prompt>
    </system_prompt>
    """

    user_prompt = f"""
    <user_prompt>
        <task>{metadata.task}</task>

        <subtasks>
        {chr(10).join(subtasks.subtasks) if subtasks.subtasks else 'None'}
        </subtasks>

        <confidence>{subtasks.confidence}</confidence>

        <concerns>
        {chr(10).join(subtasks.concerns) if subtasks.concerns else 'None'}
        </concerns>

        <questions>
        {chr(10).join(subtasks.questions) if subtasks.questions else 'None'}
        </questions>
    </user_prompt>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    import json
    try:
        content = response.choices[0].message.content.strip()
        return SubtaskJudgment(**json.loads(content))
    except Exception:
        return SubtaskJudgment(
            judgment="fail",
            reason="Subtask judgment failed: unable to parse subtask judgment response."
        )
def save_task_to_db(task: str, subtasks: Optional[List[str]] = None):
    subtasks = subtasks or []
    print(f"[SAVE] Task: {task}\n[SUBTASKS]\n" + chr(10).join(f"- {s}" for s in subtasks))
    return {
        "status": "saved"
    }

def generate_subtasks(metadata: TaskMetadata) -> SubtaskMetadata:
    """
    Use LLM to propose subtasks for a given task and identify missing information.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
        You are an expert task planning assistant. Your job is to break down a single task into clear, 
        actionable subtasks, assess how confident you are, list any concerns, and, if you want
        clarification from the user, generate clarification questions.
        
        Always respond using the following JSON format:
        {
        "subtasks": [<string>, ...],
        "confidence": <float>,
        "concerns": [<string>, ...],
        "questions": [<string>, ...]
        }
    </system_prompt>
    """

    user_prompt = f"""
    <user_prompt>
        {metadata.task}
    </user_prompt>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    try:
        content = response.choices[0].message.content.strip()
        return SubtaskMetadata(**json.loads(content))
    except Exception:
        return SubtaskMetadata(
            subtasks=[],
            confidence=0.0,
            concerns=["Unable to parse subtask generation response"],
            questions=[]
        )

def ask_clarifying_questions(questions: List[str]) -> dict:
    """
    Present clarifying questions to the user. Currently a stub.
    TODO: Implement UI interaction for asking questions
    """
    return {
        "updated_input": "User's response to questions"  # Placeholder response
    }

def create_task(task: str, subtasks: Optional[List[str]] = None) -> dict:
    """
    Create a new task with optional subtasks. Currently a stub.
    TODO: Implement task creation logic
    """
    return {
        "status": "created",
        "task": task,
        "subtasks": subtasks or []
    }

# The following functions are stubs for the v2 task agent
def create_clarifying_questions(task: str) -> dict:    
    """
    Create clarifying questions for a subtask.
    """
    return {
        "questions": [f"What is the purpose of {subtask}?" for subtask in subtasks]
    }
def receive_clarification_feedback(feedback: str, subtask: str) -> dict:
    """
    Receive feedback on a clarification question.
    """
    return {
        "feedback": feedback
    }
