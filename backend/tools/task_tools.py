from typing import List, Optional
from openai import OpenAI
import os
from dotenv import load_dotenv
import json
from backend.types import TaskMetadata, TaskJudgment, SubtaskMetadata

# Load environment variables from .env file
load_dotenv()

# --- Constants ---
DEFAULT_MODEL = "gpt-4"

# --- Shared LLM client accessor ---
def get_client():
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        raise ValueError(
            "OpenAI API key not found. Please set the OPENAI_API_KEY environment variable. "
            "You can get an API key from https://platform.openai.com/api-keys"
        )
    return OpenAI(api_key=openai_api_key)

# --- LLM-enabled functions ---
def extract_task(state) -> TaskMetadata:
    """
    Use LLM to extract the main task, assess confidence, raise concerns, and generate clarifying questions.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
        You are an expert task manager assistant.
        Your job is to extract a single task from the user's input, assess how confident you are,
        list any concerns, and, if you want clarification from the user, generate clarification questions.

        Always respond using the following JSON format:
        {
        "task": <string>,
        "confidence": <float between 0 and 1>,
        "concerns": [<string>, ...],
        "questions": [<string>, ...]
        }
    </system_prompt>
    """

    user_prompt = f"""
    <user_input>
    {state.input}
    </user_input>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    
    try:
        content = response.choices[0].message.content.strip()
        return TaskMetadata(**json.loads(content))
    except Exception as e:
        return TaskMetadata(
            task=state.input.strip(),
            confidence=0.5,
            concerns=["Failed to parse LLM response"],
            questions=[]
        )

def clarify(missing_info: List[str]) -> dict:
    """
    Generate clarification questions for missing task info using an LLM.
    """
    client = get_client()
    if not missing_info:
        return {"questions": []}

    prompt = f"""
    A task is missing the following pieces of information: {', '.join(missing_info)}.
    Write a short clarification question for each.
    """
    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": "You generate clarification questions for missing task details."},
            {"role": "user", "content": prompt}
        ]
    )
    content = response.choices[0].message.content.strip()
    questions = [line.lstrip("- ").strip() for line in content.splitlines() if line.strip()]
    return {
        "questions": questions
    }

def review(task: str, subtasks: Optional[List[str]] = None) -> dict:
    """
    Present the task and subtasks back to the user for review.
    """
    return {
        "task": task,
        "subtasks": subtasks or [],
        "message": "Here's what I've extracted. Let me know if you'd like to change anything."
    }

def judge_task(metadata: TaskMetadata) -> TaskJudgment:
    """
    Determine if the extracted task is clearly defined and actionable.
    Uses task confidence, concerns, and clarification questions as context.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
        You are a quality control specialist reviewing a proposed task generated by an AI assistant.

        Your job is to decide if the proposed task is clearly defined, actionable, and appropriately scoped.

        Consider the following:
        - Confidence score below 0.7 should make you cautious.
        - If concerns or clarification questions are present, it's more likely the task is vague.
        - However, you must use your own judgment to decide if the task can proceed or not.

        If the task is too ambiguous and no questions were suggested by the assistant, 
        you must generate at least one clarifying question for the user and include it in your reason.

        Always respond using the following JSON format:
        {
        "judgment": "pass" or "fail",
        "reason": "<clarification or explanation if needed>"
        }
        </system_prompt>
    """

    user_prompt = f"""
    <task>{metadata.task}</task>
    <confidence>{metadata.confidence}</confidence>

    <concerns>
    {chr(10).join(metadata.concerns) if metadata.concerns else 'None'}
    </concerns>

    <questions>
    {chr(10).join(metadata.questions) if metadata.questions else 'None'}
    </questions>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    
    try:
        content = response.choices[0].message.content.strip()
        return TaskJudgment(**json.loads(content))
    except Exception:
        return TaskJudgment(
            judgment="fail",
            reason="Task judgment failed: unable to parse LLM response."
        )

def judge_subtasks(original_prompt: str, final_task: str, subtasks: List[str]) -> dict:
    """
    Check if the subtasks are a reasonable decomposition of the original task.
    """
    client = get_client()
    prompt = f"""
    Original task input:
    "{original_prompt}"

    Extracted main task:
    "{final_task}"

    Proposed subtasks:
    {chr(10).join(f"- {s}" for s in subtasks)}

    Are the subtasks a reasonable breakdown of the task? Respond with "approved" or "needs revision" only.
    """
    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": "You are an expert task workflow judge."},
            {"role": "user", "content": prompt}
        ]
    )
    result = response.choices[0].message.content.strip().lower()
    return {
        "status": "approved" if "approved" in result else "needs revision"
    }

def save_task_to_db(task: str, subtasks: Optional[List[str]] = None):
    subtasks = subtasks or []
    print(f"[SAVE] Task: {task}\n[SUBTASKS]\n" + chr(10).join(f"- {s}" for s in subtasks))
    return {
        "status": "saved"
    }

def generate_subtasks(metadata: TaskMetadata) -> SubtaskMetadata:
    """
    Use LLM to propose subtasks for a given task and identify missing information.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
        You are an expert task planning assistant. Your job is to break down a single task into clear, actionable subtasks
        and identify any missing information needed to proceed.

        Return your response in this JSON format:
        {
        "subtasks": [<string>, ...],
        "missing_info": [<string>, ...]
        }
    </system_prompt>
    """

    user_prompt = f"""
    <user_prompt>
        <task>{metadata.task}</task>
        <confidence>{metadata.confidence}</confidence>
    </user_prompt>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    
    try:
        content = response.choices[0].message.content.strip()
        return SubtaskMetadata(**json.loads(content))
    except Exception:
        return SubtaskMetadata(
            subtasks=[],
            missing_info=["Unable to parse subtask generation response"]
        )

def ask_clarifying_questions(questions: List[str]) -> dict:
    """
    Present clarifying questions to the user. Currently a stub.
    TODO: Implement UI interaction for asking questions
    """
    return {
        "updated_input": "User's response to questions"  # Placeholder response
    }

def create_task(task: str, subtasks: Optional[List[str]] = None) -> dict:
    """
    Create a new task with optional subtasks. Currently a stub.
    TODO: Implement task creation logic
    """
    return {
        "status": "created",
        "task": task,
        "subtasks": subtasks or []
    }

# The following functions are stubs for the v2 task agent
def ask_to_subtask(task: str) -> dict:
    """
    Ask the user to select a subtask from a list.
    """
    return {
        "decision": "no"
    }
def create_clarifying_questions(task: str) -> dict:    
    """
    Create clarifying questions for a subtask.
    """
    return {
        "questions": [f"What is the purpose of {subtask}?" for subtask in subtasks]
    }
def receive_clarification_feedback(feedback: str, subtask: str) -> dict:
    """
    Receive feedback on a clarification question.
    """
    return {
        "feedback": feedback
    }
