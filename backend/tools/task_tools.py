from typing import List, Optional
from openai import OpenAI
import os
from dotenv import load_dotenv
import json
from backend.types import TaskMetadata, TaskJudgment, SubtaskMetadata, SubtaskJudgment

# Load environment variables from .env file
load_dotenv()

# --- Constants ---
DEFAULT_MODEL = "gpt-4"

# --- Shared LLM client accessor ---
def get_client():
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        raise ValueError(
            "OpenAI API key not found. Please set the OPENAI_API_KEY environment variable. "
            "You can get an API key from https://platform.openai.com/api-keys"
        )
    return OpenAI(api_key=openai_api_key)

# --- LLM-enabled functions ---
def extract_task(state) -> TaskMetadata:
    """
    Use LLM to extract the main task, assess confidence, raise concerns, and generate clarifying questions.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
        You are an expert task manager assistant.
        Your job is to extract a single task from the user's input, assess how confident you are,
        list any concerns, and, if you want clarification from the user, generate clarification questions.

        Always respond using the following JSON format:
        {
        "task": <string>,
        "confidence": <float between 0 and 1>,
        "concerns": [<string>, ...],
        "questions": [<string>, ...]
        }
    </system_prompt>
    """

    user_prompt = f"""
    <user_prompt>
        {state.input}
    </user_prompt>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    try:
        content = response.choices[0].message.content.strip()
        return TaskMetadata(**json.loads(content))
    except Exception as e:
        return TaskMetadata(
            task=state.input.strip(),
            confidence=0.0,
            concerns=["Unable to parse task extraction response"],
            questions=[]
        )

def judge_task(metadata: TaskMetadata) -> TaskJudgment:
    """
    Determine if the extracted task is clearly defined and actionable.
    Uses task confidence, concerns, and clarification questions as context.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
        You are a quality control specialist reviewing a proposed task generated by an AI assistant.

        Your job is to decide if the proposed task is clearly defined, actionable, and appropriately scoped.

        Consider the following:
        - Confidence score below 0.7 should make you cautious.
        - If concerns or clarification questions are present, it's more likely the task is vague.
        - However, you must use your own judgment to decide if the task can proceed or not.

        If the task is too ambiguous and no questions were suggested by the assistant, 
        you must generate at least one clarifying question for the user and include it in your reason.

        Always respond using the following JSON format:
        {
        "judgment": "pass" or "fail",
        "reason": "<clarification or explanation if needed>"
        }
        </system_prompt>
    """

    user_prompt = f"""
    <user_prompt>
        <task>{metadata.task}</task>
        <confidence>{metadata.confidence}</confidence>

        <concerns>
        {chr(10).join(metadata.concerns) if metadata.concerns else 'None'}
        </concerns>

        <questions>
        {chr(10).join(metadata.questions) if metadata.questions else 'None'}
        </questions>
    </user_prompt>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    try:
        content = response.choices[0].message.content.strip()
        return TaskJudgment(**json.loads(content))
    except Exception:
        return TaskJudgment(
            judgment="fail",
            reason="Task judgment failed: unable to parse task judgment response."
        )

def judge_subtasks(metadata: TaskMetadata, subtasks: SubtaskMetadata) -> SubtaskJudgment:
    """
    Evaluate whether the generated subtasks represent a complete and logical decomposition of the main task.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
        You are a quality control specialist reviewing a proposed list of subtasks generated by an AI assistant.
        Your job is to determine if a list of subtasks correctly and completely decomposes the main task.

        If the subtasks are incomplete, overlapping, redundant, or not well-aligned with the task, return "fail" and explain why.

        Consider the following:
        - Confidence score below 0.7 should make you cautious.
        - If concerns or clarification questions are present, it's more likely the subtasks are vague and/or incomplete.
        - However, you must use your own judgment to decide if the task can proceed or not.

        If the subtasks are too ambiguous and no questions were suggested by the assistant, you 
        must generate at least one clarifying question for the user and include it in your reason.

        Always respond using the following JSON format:
        {
        "judgment": "pass" or "fail",
        "reason": "<clarification or explanation if needed>"
        }
    </system_prompt>
    """

    user_prompt = f"""
    <user_prompt>
        <task>{metadata.task}</task>

        <subtasks>
        {chr(10).join(subtasks.subtasks) if subtasks.subtasks else 'None'}
        </subtasks>

        <confidence>{subtasks.confidence}</confidence>

        <concerns>
        {chr(10).join(subtasks.concerns) if subtasks.concerns else 'None'}
        </concerns>

        <questions>
        {chr(10).join(subtasks.questions) if subtasks.questions else 'None'}
        </questions>
    </user_prompt>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    try:
        content = response.choices[0].message.content.strip()
        return SubtaskJudgment(**json.loads(content))
    except Exception:
        return SubtaskJudgment(
            judgment="fail",
            reason="Subtask judgment failed: unable to parse subtask judgment response."
        )

def save_task_to_db(task: str, subtasks: Optional[List[str]] = None):
    subtasks = subtasks or []
    print(f"[SAVE] Task: {task}\n[SUBTASKS]\n" + chr(10).join(f"- {s}" for s in subtasks))
    return {
        "status": "saved"
    }

def generate_subtasks(metadata: TaskMetadata) -> SubtaskMetadata:
    """
    Use LLM to propose subtasks for a given task and identify missing information.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
        You are an expert task planning assistant. Your job is to break down a single task into clear, 
        actionable subtasks, assess how confident you are, list any concerns, and, if you want
        clarification from the user, generate clarification questions.
        
        Always respond using the following JSON format:
        {
        "subtasks": [<string>, ...],
        "confidence": <float>,
        "concerns": [<string>, ...],
        "questions": [<string>, ...]
        }
    </system_prompt>
    """

    user_prompt = f"""
    <user_prompt>
        {metadata.task}
    </user_prompt>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    try:
        content = response.choices[0].message.content.strip()
        return SubtaskMetadata(**json.loads(content))
    except Exception:
        return SubtaskMetadata(
            subtasks=[],
            confidence=0.0,
            concerns=["Unable to parse subtask generation response"],
            questions=[]
        )

def create_task(task: str, subtasks: Optional[List[str]] = None) -> dict:
    """
    Create a new task with optional subtasks.
    Currently just logs the task and subtasks.
    TODO: Implement actual task storage
    """
    subtasks = subtasks or []
    print(f"[SAVE] Task: {task}\n[SUBTASKS]\n" + chr(10).join(f"- {s}" for s in subtasks))
    return {
        "status": "saved",
        "task": task,
        "subtasks": subtasks
    }

def generate_task_clarification_prompt(metadata, judgment, context_type: str) -> str:
    """
    Generate a human-friendly prompt for task/subtask clarification.
    Uses concerns and questions from metadata to create a clear message.
    """
    concerns = metadata.concerns if metadata.concerns else []
    questions = metadata.questions if metadata.questions else []
    
    prompt = f"I need some clarification about your {context_type}.\n\n"
    
    if concerns:
        prompt += "I have some concerns:\n"
        for concern in concerns:
            prompt += f"- {concern}\n"
        prompt += "\n"
    
    if questions:
        prompt += "Could you please clarify:\n"
        for question in questions:
            prompt += f"- {question}\n"
    
    if not concerns and not questions:
        prompt += f"Could you please provide more details about your {context_type}?\n"
    
    return prompt

def retry_task_with_feedback(state) -> TaskMetadata:
    """
    Use LLM to refine the task based on user feedback.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
        You are an expert task manager assistant.
        Your job is to refine the task based on the user's feedback.
        Consider the feedback carefully and update the task accordingly.
        Assess your confidence in the refined task, list any remaining concerns,
        and generate any additional clarification questions if needed.

        Always respond using the following JSON format:
        {
        "task": <string>,
        "confidence": <float between 0 and 1>,
        "concerns": [<string>, ...],
        "questions": [<string>, ...]
        }
    </system_prompt>
    """

    user_prompt = f"""
    <user_prompt>
        <original_task>{state.task_metadata.task}</original_task>
        <user_feedback>{state.user_feedback}</user_feedback>
    </user_prompt>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    try:
        content = response.choices[0].message.content.strip()
        return TaskMetadata(**json.loads(content))
    except Exception as e:
        return TaskMetadata(
            task=state.task_metadata.task,
            confidence=0.0,
            concerns=["Unable to parse task refinement response"],
            questions=[]
        )

def retry_subtasks_with_feedback(state) -> SubtaskMetadata:
    """
    Use LLM to refine subtasks based on user feedback.
    """
    client = get_client()

    system_msg = """
    <system_prompt>
        You are an expert task planning assistant.
        Your job is to refine the subtasks based on the user's feedback.
        Consider the feedback carefully and update the subtasks accordingly.
        Assess your confidence in the refined subtasks, list any remaining concerns,
        and generate any additional clarification questions if needed.
        
        Always respond using the following JSON format:
        {
        "subtasks": [<string>, ...],
        "confidence": <float>,
        "concerns": [<string>, ...],
        "questions": [<string>, ...]
        }
    </system_prompt>
    """

    user_prompt = f"""
    <user_prompt>
        <task>{state.task_metadata.task}</task>
        <user_feedback>{state.user_feedback}</user_feedback>
    </user_prompt>
    """

    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    try:
        content = response.choices[0].message.content.strip()
        return SubtaskMetadata(**json.loads(content))
    except Exception:
        return SubtaskMetadata(
            subtasks=[],
            confidence=0.0,
            concerns=["Unable to parse subtask refinement response"],
            questions=[]
        )
